{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visit the Wikipedia hyperlinks graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to perform an analysis of the Wikipedia Hyperlink graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from functions import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we import the **wiki-topcats-reduced.txt** file where each row represents an edge (departure node and arrival one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>401135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>1069112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>1163551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>12162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>167659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from       to\n",
       "0    52   401135\n",
       "1    52  1069112\n",
       "2    52  1163551\n",
       "3    62    12162\n",
       "4    62   167659"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('wiki-topcats-reduced.txt', sep = '\\t', header = None)\n",
    "df.columns = ['from', 'to']\n",
    "names = name_creation()\n",
    "names = pd.DataFrame.from_dict(names, orient='index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   from       to\n",
      "0    52   401135\n",
      "1    52  1069112\n",
      "2    52  1163551 \n",
      "\n",
      "          from      to\n",
      "361754  401135   60219\n",
      "361755  401135  167532\n",
      "361756  401135  400980\n",
      "361757  401135  401018\n",
      "361758  401135  401019\n"
     ]
    }
   ],
   "source": [
    "# directed cause these two are different\n",
    "print(df[df['from'] ==52], '\\n')\n",
    "print(df[df['from'] ==401135].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking a look at the dataframe we notice that the graph is **directed**: in fact the edges are directed from one vertex to another. As we can see in the dataframe, a node could have edges departing from it but no edges to it. In the example presented above, we've seen that the node $52$ goes to the node $401135$ but the inverse edge is not present.\n",
    "\n",
    "Since the graph is directed we cannot consider only one column of the dataframe thus the number of nodes is given by the length of the *set* of both columns while the number of edges is simply the length of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 461193\n",
      "Number of edges: 2645247\n"
     ]
    }
   ],
   "source": [
    "#vertices\n",
    "\n",
    "fr = df.groupby('from').count()\n",
    "to = df.groupby('to').count()\n",
    "V = set(list(fr.index) + list(to.index))\n",
    "print('Number of nodes =', len(V))\n",
    "print('Number of edges:', len(df.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average degree of the vertex is: 5.7357\n"
     ]
    }
   ],
   "source": [
    "print('The average degree of the vertex is:', round(len(df.index)/len(V), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found that the average degree of all the vertexes is $5.74$ and this means that the graph is sparse. \n",
    "The average degree has been found with the formula:\n",
    "\n",
    "$average\\_degree$ = $\\frac{total\\_edges}{total\\_vertex}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can work on the **wikipedia categories** and drop those with a number of articles less than $3500$ and create two dictionaries that have the same structure:\n",
    "* categories = {category_name : nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = category_creation(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **categories** dictionary contains the nodes that are present in the initial dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is asked to the user to insert an initial category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year_of_birth_unknown\n"
     ]
    }
   ],
   "source": [
    "input_category = input()\n",
    "'Year_of_birth_unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DG = nx.from_pandas_edgelist(df, 'from', 'to', create_using = nx.DiGraph )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find all the shortest paths from the input category to all the others, we have used the **BFS** algorithm. \n",
    "We have runned this algorithm for each node present in the initial category. At the end we have a dictionary that has as keys the nodes that have been visited with all the associated shortest paths.\n",
    "An example of the final dictionary is shown here:\n",
    "\n",
    "$\\{node_i: [dist_1, dist_2,..., dist_n ]\\}$ , where n is the number of nodes in the input category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2536/2536 [1:16:34<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "bfs = BFS(DG, categories, input_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the process described above takes a long time, so, in order to save time, we have saved the data that can be accessed faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median = median_calculation(categories,input_category, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Year_of_birth_unknown', -1),\n",
       "             ('English-language_films', 6.0),\n",
       "             ('American_film_actors', 6.0),\n",
       "             ('Members_of_the_United_Kingdom_Parliament_for_English_constituencies',\n",
       "              7.0),\n",
       "             ('Indian_films', 7.0),\n",
       "             ('English_television_actors', 7.0),\n",
       "             ('British_films', 7.0),\n",
       "             ('American_films', 7.0),\n",
       "             ('American_Jews', 7.0),\n",
       "             ('American_television_actors', 7.0),\n",
       "             ('Black-and-white_films', 7.0),\n",
       "             ('Article_Feedback_Pilot', 7.0),\n",
       "             ('Rivers_of_Romania', 8.0),\n",
       "             ('English-language_albums', 8.0),\n",
       "             ('People_from_New_York_City', 8.0),\n",
       "             ('Debut_albums', 9.0),\n",
       "             ('English_footballers', 10000),\n",
       "             ('The_Football_League_players', 10000),\n",
       "             ('Association_football_forwards', 10000),\n",
       "             ('Association_football_goalkeepers', 10000),\n",
       "             ('Association_football_midfielders', 10000),\n",
       "             ('Association_football_defenders', 10000),\n",
       "             ('Living_people', 10000),\n",
       "             ('Harvard_University_alumni', 10000),\n",
       "             ('Major_League_Baseball_pitchers', 10000),\n",
       "             ('Year_of_death_missing', 10000),\n",
       "             ('English_cricketers', 10000),\n",
       "             ('Year_of_birth_missing_(living_people)', 10000),\n",
       "             ('Main_Belt_asteroids', 10000),\n",
       "             ('Asteroids_named_for_people', 10000),\n",
       "             ('Fellows_of_the_Royal_Society', 10000),\n",
       "             ('Year_of_birth_missing', 10000),\n",
       "             ('Place_of_birth_missing_(living_people)', 10000),\n",
       "             ('American_military_personnel_of_World_War_II', 10000),\n",
       "             ('Windows_games', 10000)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_ranking = OrderedDict()\n",
    "block_ranking = OrderedDict(sorted(median.items(), key=lambda x: x[1]))\n",
    "block_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **articles** dictionary contains the articles and the corresponding categories in which it appears:\n",
    "* articles = {article : list_of_categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each article to the categories it is present\n",
    "\n",
    "articles = {}\n",
    "for name in categories:\n",
    "    for article in categories[name]:\n",
    "        if article in articles:\n",
    "            articles[article].append(name)\n",
    "        else:\n",
    "            articles[article] = [name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can happen that an article belongs to several categories: in this case we assign it to the input category if the input category is one of these otherwise we choose the category according to block_ranking, i.e. it will belong to the category closest to the input one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if an article belongs to multiple categories, choose one according to block_ranking\n",
    "\n",
    "for article in articles:\n",
    "    if len(articles[article]) > 1:\n",
    "        minimum = ''\n",
    "        for cat in articles[article]:\n",
    "            if (minimum == '') or (block_ranking[minimum] > block_ranking[cat]):\n",
    "                minimum = cat\n",
    "        articles[article] = [minimum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary **categories_after_ranking** has the category as key and a list of articles belonging to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_after_ranking = {}\n",
    "\n",
    "for i in articles:  # article??\n",
    "    if articles[i][0] in categories_after_ranking:\n",
    "        categories_after_ranking[articles[i][0]] +=  [i]\n",
    "    else:\n",
    "        categories_after_ranking[articles[i][0]] =  [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a rank to all the categories, now it is time to assign a rank to all the nodes. This is possible working with subgraphs and weights of the edges.\n",
    "\n",
    "We have followed these three steps:\n",
    "\n",
    "*  Starting from the input category, C\\0 we have computed its subgraph. Then the weights of all the edges departing from a node of the input category have been updated with the corresponding weight of that node.\n",
    "* After this, we have computed the subgraph induced by the second category, according to the block ranking, that gives an initial weight to all the nodes in that category. All the weights are updated with those that belong to the in-edges that come from the input category.\n",
    "* Now the step 2 is repeated for all the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [00:29<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "weight_dict = subgraph_calculation(DG, block_ranking, categories_after_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank per ogni nodo (come nello step3 dell'homework)\n",
    "rank = []\n",
    "for i in weight_dict:\n",
    "    rank += list(OrderedDict(sorted(weight_dict[i].items(), key = lambda x: x[1], reverse = True)).keys())\n",
    "\n",
    "ranks = {}\n",
    "for i in range(50):\n",
    "    ranks[rank[i]] = names.loc[rank[i]].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62684</th>\n",
       "      <td>Diogenes Lartius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170163</th>\n",
       "      <td>Pausanias (geographer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656777</th>\n",
       "      <td>L Bu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656780</th>\n",
       "      <td>Dong Zhuo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169696</th>\n",
       "      <td>Theocritus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656794</th>\n",
       "      <td>Yuan Shu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170578</th>\n",
       "      <td>Stobaeus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342864</th>\n",
       "      <td>Penda of Mercia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343014</th>\n",
       "      <td>Symeon of Durham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656778</th>\n",
       "      <td>Zhang Fei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Article_name\n",
       "62684          Diogenes Lartius\n",
       "170163   Pausanias (geographer)\n",
       "1656777                    L Bu\n",
       "1656780               Dong Zhuo\n",
       "169696               Theocritus\n",
       "1656794                Yuan Shu\n",
       "170578                 Stobaeus\n",
       "1342864         Penda of Mercia\n",
       "1343014        Symeon of Durham\n",
       "1656778               Zhang Fei"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(ranks, orient='index', columns=['Article_name']).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
